{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from totensor import ToTensor\n",
    "from PIL import Image\n",
    "from torchvision import get_image_backend\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_loader(video_dir_path,frame_indices):\n",
    "    video = []\n",
    "    for i in frame_indices:\n",
    "        image_path = os.path.join(video_dir_path,'image_{:05d}.jpg'.format(i))\n",
    "        if os.path.exists(image_path):\n",
    "            with open(image_path,'rb') as f:\n",
    "                with Image.open(f) as img:\n",
    "                    video.append(img.convert('RGB'))\n",
    "        else:\n",
    "            return video\n",
    "    return video\n",
    "        \n",
    "def load_value_file(file_path):\n",
    "    with open(file_path, 'r') as input_file:\n",
    "        value = float(input_file.read().rstrip('\\n\\r'))\n",
    "\n",
    "    return value\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the Json File Entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('activity_net.v1-3.min.json','r') as data_file:\n",
    "    dataa =json.load(data_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in dataa['database'].items():\n",
    "    print(key)\n",
    "    print(value['annotations'][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('3drKinetics.json','r') as data_file:\n",
    "    data = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in data['database'].items():\n",
    "    print(key)\n",
    "    print(value['annotations'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make data from video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(root_path,\n",
    "                 annotation_path, \n",
    "                 subset,\n",
    "                 n_samples_for_each_video, \n",
    "                 sample_duration,\n",
    "                 dataset_name):\n",
    "    \n",
    "    # Load annotation data\n",
    "    with open(annotation_path,'r') as data_file:\n",
    "        data = json.load(data_file)\n",
    "\n",
    "    # get video names and annotations\n",
    "    video_names = []\n",
    "    annotations = []\n",
    "\n",
    "    if dataset_name =='activitynet':\n",
    "\n",
    "        for key, value in data['database'].items():\n",
    "            this_subset = value['subset']\n",
    "            if this_subset == subset:\n",
    "                if subset == 'testing':\n",
    "                    video_names.append('v_{}'.format(key))\n",
    "                else:\n",
    "                    video_names.append('v_{}'.format(key))\n",
    "                    annotations.append(value['annotations'])\n",
    "        \n",
    "        class_names = []\n",
    "        index = 0\n",
    "        for node1 in data['taxonomy']:\n",
    "            is_leaf = True\n",
    "            for node2 in data['taxonomy']:\n",
    "                if node2['parentId'] == node1['nodeId']:\n",
    "                    is_leaf = False\n",
    "                    break\n",
    "            if is_leaf:\n",
    "                class_names.append(node1['nodeName'])\n",
    "\n",
    "        class_to_idx = {}\n",
    "\n",
    "        for i, class_name in enumerate(class_names):\n",
    "            class_to_idx[class_name] = i\n",
    "\n",
    "\n",
    "    elif dataset_name =='kinetics':\n",
    "            \n",
    "        for key, value in data['database'].items():\n",
    "            this_subset = value['subset']\n",
    "            if this_subset == subset:\n",
    "                if subset == 'test':\n",
    "                    video_names.append('test/{}'.format(key))\n",
    "                else:\n",
    "\n",
    "                    label = value['annotations']['label']\n",
    "                    video_names.append('{}/{}'.format(label, key))\n",
    "            \n",
    "                    annotations.append([value['annotations']])\n",
    "                    #('{}/{}'.format(label,key),value['annotations'])\n",
    "\n",
    "    \n",
    "        # compute class to label ids \n",
    "        class_to_idx ={}\n",
    "        index = 0\n",
    "        for class_label in data['labels']:\n",
    "            class_to_idx[class_label] = index\n",
    "            index +=1\n",
    "\n",
    "        \n",
    "    # compute label to class ids\n",
    "    idx_to_class ={}\n",
    "    for name,label in class_to_idx.items():\n",
    "        idx_to_class[label] = name\n",
    "        \n",
    "    dataset = []\n",
    "    for i in range(len(video_names)):\n",
    "        if i % 1000 == 0:\n",
    "            ('dataset loading [{}/{}]'.format(i, len(video_names)))\n",
    "\n",
    "        video_path = os.path.join(root_path, video_names[i])\n",
    "        (video_path)\n",
    "        if not os.path.exists(video_path):\n",
    "            continue\n",
    "        \n",
    "\n",
    "        file_names = os.listdir(video_path)\n",
    "        image_file_names = [x for x in file_names if 'image' in x]\n",
    "        image_file_names.sort(reverse=True)\n",
    "        n_frames = int(image_file_names[0][6:11])\n",
    "\n",
    " #       n_frames_file_path = os.path.join(video_path, 'n_frames')\n",
    " #       n_frames = int(load_value_file(n_frames_file_path))\n",
    "        if n_frames <= 0:\n",
    "            continue\n",
    "            \n",
    "        for annotation in annotations[i]:\n",
    "            \n",
    "            \n",
    "            if dataset_name == 'activitynet':\n",
    "                begin_t = 1 # math.ceil(annotation['segment'][0] * fps)\n",
    "                end_t = n_frames #math.ceil(annotation['segment'][1] * fps)\n",
    "                video_id = video_names[i][2:]\n",
    "                \n",
    "            elif dataset_name == 'kinetics':\n",
    "                begin_t = 1\n",
    "                end_t = n_frames\n",
    "                video_id = video_names[i][:-14].split('/')[1]\n",
    "\n",
    "            sample = {\n",
    "                'video': video_path,\n",
    "                'segment': [begin_t, end_t],\n",
    "                'n_frames': n_frames,\n",
    "                'video_id': video_id\n",
    "            }\n",
    "            \n",
    "            \n",
    "            if len(annotations) != 0:\n",
    "                (annotation)\n",
    "                sample['label'] = class_to_idx[annotation['label']]\n",
    "            else:\n",
    "                sample['label'] = -1\n",
    "\n",
    "\n",
    "\n",
    "            if n_samples_for_each_video == 1:\n",
    "                (\"inside n_smaples for each video\")\n",
    "                sample['frame_indices'] = list(range(1, n_frames + 1))\n",
    "                ()\n",
    "                \n",
    "                dataset.append(sample)\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                if n_samples_for_each_video > 1:\n",
    "                    step = max(1,\n",
    "                               math.ceil((n_frames - 1 - sample_duration) /\n",
    "                                         (n_samples_for_each_video - 1)))\n",
    "                    (n_frames,step)\n",
    "                else:\n",
    "                    step = sample_duration\n",
    "                for j in range(1, n_frames, step):\n",
    "                    sample_j = copy.deepcopy(sample)\n",
    "                    sample_j['frame_indices'] = list(\n",
    "                        range(j, min(n_frames + 1, j + sample_duration)))\n",
    "                    dataset.append(sample_j)\n",
    "\n",
    "    return dataset, idx_to_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kinetics Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self,\n",
    "                root_path,\n",
    "                annotation_path,\n",
    "                subset,\n",
    "                n_samples_for_each_video = 1,\n",
    "                sample_duration = 16,\n",
    "                dataset_name = 'activitynet',\n",
    "                get_loader = video_loader):\n",
    "        self.data,self.class_names = make_dataset(\n",
    "                                    root_path,annotation_path,subset,\n",
    "                                    n_samples_for_each_video,\n",
    "                                    sample_duration,\n",
    "                                    dataset_name)\n",
    "        self.loader = get_loader\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        \n",
    "        path = self.data[index]['video']\n",
    "        \n",
    "        frame_indices = self.data[index]['frame_indices']\n",
    "        \n",
    "        clip = self.loader(path,frame_indices)\n",
    "        clip = [ToTensor(1)(img) for img in clip]\n",
    "        clip = torch.stack(clip,0).permute(1,0,2,3)\n",
    "        \n",
    "        target = self.data[index]\n",
    "        \n",
    "        return clip , target\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For ActivityNet\n",
    "training_data = VideoDataset('ActivityNet_JPG/','activity_net.v1-3.min.json','validation')\n",
    "#why it does not work??? \n",
    "#at is the problem?\n",
    "\n",
    "# For Kinetics\n",
    "#raining_data = VideoDataset('Kinetics_JPG/','3drKinetics.json','training',dataset_name='kinetics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    training_data,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers = 4,\n",
    "    pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (inputs,targets) in train_loader:\n",
    "    print(inputs)\n",
    "    print(targets)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
